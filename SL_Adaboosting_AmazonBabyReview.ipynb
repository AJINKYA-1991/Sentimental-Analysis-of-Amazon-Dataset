{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AdaBoosting with Decision Tree Amazon Baby Review\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.120430078052725\n",
      "1.2853703237434095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#reading reviews using pandas library from amazon_baby_train.csv file\n",
    "reviews = pd.read_csv('amazon_baby_train.csv')\n",
    "reviews.shape\n",
    "\n",
    "# dropping observations which are incomplete\n",
    "reviews = reviews.dropna()\n",
    "reviews.shape\n",
    "\n",
    "# changing the reviews into positive and negative reviews\n",
    "scores = reviews['rating']\n",
    "reviews['rating'] = reviews['rating'].apply(lambda x: 'pos' if x > 3 else 'neg')\n",
    "\n",
    "# printing the mean and standard deviation of ratings\n",
    "print(scores.mean())\n",
    "print(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "neg     34398\n",
       "pos    111529\n",
       "Name: review, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grouping the reviews into positive and negative\n",
    "reviews.groupby('rating')['review'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a8c8a566d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAGMCAYAAACI+5UEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIQ2AIABCG0AAAMQ2gAABvD/Ab3BjQPl4JdoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a8c566f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting a graph which counts the number of positive and negative labels\n",
    "reviews.groupby('rating')['review'].count().plot(kind='bar', color= ['r','g'],title='Label Distribution',  figsize = (10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the positive and negative review and storing them in separate arrays\n",
    "def splitPosNeg(Summaries):\n",
    "    neg = reviews.loc[Summaries['rating']== 'neg']\n",
    "    pos = reviews.loc[Summaries['rating']== 'pos']\n",
    "    return [pos,neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the positive and negative review and storing them in separate arrays\n",
    "[pos,neg] = splitPosNeg(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using lemmatizer to lemmatizze words\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# using stop words to remove the words which do not contribute to the sentiment\n",
    "stops = stopwords.words('english')\n",
    "stops.remove('not')\n",
    "stops.remove('no')\n",
    "translation = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = line.translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())   \n",
    "    for t in line:\n",
    "        stemmed = lemmatizer.lemmatize(t)\n",
    "        tokens.append(stemmed)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Storing the positive and negative reviews in separate arrays\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "for p in pos['review']:\n",
    "    pos_data.append(preprocessing(p))\n",
    "\n",
    "for n in neg['review']:\n",
    "    neg_data.append(preprocessing(n))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combining the positive and negative reviews\n",
    "data = pos_data + neg_data\n",
    "labels = np.concatenate((pos['rating'].values,neg['rating'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into training set and validation set\n",
    "#[Data_train,Data_test,Train_labels,Test_labels] = train_test_split(data,labels , test_size=0.25, random_state=20160121,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizing each sentence from the file into words\n",
    "t = []\n",
    "for line in data:\n",
    "    l = nltk.word_tokenize(line)\n",
    "    for w in l:\n",
    "        t.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55669\n"
     ]
    }
   ],
   "source": [
    "# Calculating the frequency dstribution of each word\n",
    "word_features = nltk.FreqDist(t)\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 611968), ('it', 399752), ('i', 375573), ('and', 365315), ('a', 340757), ('to', 334813), ('is', 205617), ('this', 178855), ('for', 173482), ('my', 147970), ('of', 146494), ('in', 144299), ('that', 121925), ('with', 100826), ('on', 100723), ('wa', 93101), ('but', 92952), ('have', 92851), ('we', 91151), ('t', 88530), ('so', 83602), ('not', 80912), ('s', 77136), ('you', 73961), ('baby', 70749)]\n"
     ]
    }
   ],
   "source": [
    "# The most common 5000 words\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(5000))]\n",
    "print(word_features.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#printing the top 200 most common words\n",
    "word_his = pd.DataFrame(word_features.most_common(200), columns = ['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3135)\t1\n",
      "  (0, 310)\t1\n",
      "  (0, 3691)\t1\n",
      "  (0, 1567)\t1\n",
      "  (0, 4520)\t1\n",
      "  (0, 2245)\t1\n",
      "  (0, 2643)\t1\n",
      "  (0, 2453)\t1\n",
      "  (0, 1875)\t1\n",
      "  (0, 3149)\t1\n",
      "  (0, 1405)\t1\n",
      "  (0, 207)\t1\n",
      "  (0, 3664)\t1\n",
      "  (0, 2562)\t1\n",
      "  (0, 1239)\t1\n",
      "  (0, 3795)\t1\n",
      "  (0, 892)\t1\n",
      "  (0, 2825)\t1\n",
      "  (0, 4445)\t1\n",
      "  (0, 3340)\t1\n",
      "  (0, 3103)\t1\n",
      "  (0, 4963)\t1\n",
      "  (0, 1721)\t1\n",
      "  (0, 3634)\t1\n",
      "  (0, 3111)\t1\n",
      "  :\t:\n",
      "  (0, 4708)\t1\n",
      "  (0, 4402)\t1\n",
      "  (0, 2914)\t1\n",
      "  (0, 307)\t1\n",
      "  (0, 374)\t1\n",
      "  (0, 4944)\t1\n",
      "  (0, 2853)\t1\n",
      "  (0, 3956)\t1\n",
      "  (0, 4787)\t1\n",
      "  (0, 2010)\t1\n",
      "  (0, 674)\t1\n",
      "  (0, 4739)\t1\n",
      "  (0, 2912)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 4383)\t1\n",
      "  (0, 2157)\t1\n",
      "  (0, 2896)\t1\n",
      "  (0, 2764)\t1\n",
      "  (0, 1748)\t1\n",
      "  (0, 4415)\t1\n",
      "  (0, 2264)\t1\n",
      "  (0, 4464)\t1\n",
      "  (0, 253)\t1\n",
      "  (0, 2271)\t1\n",
      "  (0, 4386)\t1\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the top words\n",
    "vec = CountVectorizer()\n",
    "c_fit = vec.fit_transform([' '.join(topwords)])\n",
    "print(c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Tfidf Transformer on the data\n",
    "tf_vec = TfidfTransformer()\n",
    "tf_fit = tf_vec.fit_transform(c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming the features using Tfidf transformer\n",
    "ctr_features = vec.transform(data)\n",
    "tr_features = tf_vec.transform(ctr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145927, 4967)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cte_features = vec.transform(Data_test)\n",
    "#te_features = tf_vec.transform(cte_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using AdaBoost classifier to classify the data\n",
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=50),n_estimators=120, learning_rate=0.1)\n",
    "clf = clf.fit(tr_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7709ce87af85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#predicting the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtfPredication\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtfAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfPredication\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy on the Training dataset:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfAccuracy\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\nikhil\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\nikhil\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\nikhil\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 3]"
     ]
    }
   ],
   "source": [
    "#predicting the output\n",
    "tfPredication = clf.predict(tr_features)\n",
    "tfAccuracy = metrics.accuracy_score(tfPredication,labels)\n",
    "print(\"Accuracy on the Training dataset:\")\n",
    "print(tfAccuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printing the metrics\n",
    "print(metrics.classification_report(labels, tfPredication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reading reviews using pandas library from amazon_baby_test.csv file\n",
    "reviews = pd.read_csv('amazon_baby_test.csv')\n",
    "reviews.shape\n",
    "\n",
    "# dropping observations which are incomplete\n",
    "reviews = reviews.dropna()\n",
    "reviews.shape\n",
    "\n",
    "# changing the reviews into positive and negative reviews\n",
    "scores = reviews['rating']\n",
    "reviews['rating'] = reviews['rating'].apply(lambda x: 'pos' if x > 3 else 'neg')\n",
    "\n",
    "# calculating the mean of reviews\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grouping the reviews into positive and negative\n",
    "reviews.groupby('rating')['review'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a graph which counts the number of positive and negative labels\n",
    "reviews.groupby('rating')['review'].count().plot(kind='bar', color= ['r','g'],title='Label Distribution',  figsize = (10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the positive and negative review and storing them in separate arrays\n",
    "[pos,neg] = splitPosNeg(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Storing the positive and negative reviews in separate arrays\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "for p in pos['review']:\n",
    "    pos_data.append(preprocessing(p))\n",
    "\n",
    "for n in neg['review']:\n",
    "    neg_data.append(preprocessing(n))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining the positive and negative reviews\n",
    "data = pos_data + neg_data\n",
    "labels = np.concatenate((pos['rating'].values,neg['rating'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing each sentence from the file into words\n",
    "t = []\n",
    "for line in data:\n",
    "    l = nltk.word_tokenize(line)\n",
    "    for w in l:\n",
    "        t.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating the frequency dstribution of each word\n",
    "word_features = nltk.FreqDist(t)\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The most common 5002 words\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(5002))]\n",
    "print(word_features.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#printing the top 200 most common words\n",
    "word_his = pd.DataFrame(word_features.most_common(200), columns = ['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorizing the top words\n",
    "vec = CountVectorizer()\n",
    "c_fit = vec.fit_transform([' '.join(topwords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Tfidf Transformer on the data\n",
    "tf_vec = TfidfTransformer()\n",
    "tf_fit = tf_vec.fit_transform(c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming the features using Tfidf transformer\n",
    "cte_features = vec.transform(data)\n",
    "te_features = tf_vec.transform(cte_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predicting the output\n",
    "tePredication = clf.predict(te_features)\n",
    "teAccuracy = metrics.accuracy_score(tePredication,labels)\n",
    "print(teAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printing the metrics\n",
    "print(metrics.classification_report(labels, tePredication))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
