
import pandas as pd
import numpy as np
import nltk
import string
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import scipy.sparse as sparse
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cross_validation import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from nltk.corpus import stopwords
from sklearn.tree import DecisionTreeClassifier
# training data
reviews = pd.read_csv('C:\Users\Ajinkya\Desktop/amazon_baby_train.csv')
reviews.shape
reviews = reviews.dropna()
reviews.shape
print(reviews.head(25))
scores = reviews['rating']
reviews['rating'] = reviews['rating'].apply(lambda x: 'pos' if x > 3 else 'neg')
#print(reviews.head(25))
print(scores.mean())
print(scores.std())
reviews.groupby('rating')['review'].count()
reviews.groupby('rating')['review'].count().plot(kind='bar', color= ['r','g'],title='Label Distribution',  figsize = (10,6))
def splitPosNeg(Summaries):
        neg = reviews.loc[Summaries['rating']== 'neg']
        pos = reviews.loc[Summaries['rating']== 'pos']
        return [pos,neg]
[pos,neg] = splitPosNeg(reviews)
    #preprocessing steps
    #stemmer = PorterStemmer()
lemmatizer = nltk.WordNetLemmatizer()
stop = stopwords.words('english')
translation = string.maketrans(string.punctuation,' '*len(string.punctuation))
    #filtered_words = [word for word in word_list if word not in stopwords.words('english')]
def preprocessing(line):
        tokens=[]
        line = line.translate(translation)
        line = nltk.word_tokenize(line.lower())
        #print(line)
        stops = stopwords.words('english')
        stops.remove('not')
        stops.remove('no')
        line = [word for word in line if word not in stops]
        #print(\After removing stop words\)
        #print(line)
        for t in line:
            #if(t not in stop):
                #stemmed = stemmer.stem(t)
            stemmed = lemmatizer.lemmatize(t)
            tokens.append(stemmed)
        return ' '.join(tokens)
pos_data = []
neg_data = []
for p in pos['review']:
   pos_data.append(preprocessing(p))
for n in neg['review']:
    neg_data.append(preprocessing(n))
print('Done')
data = pos_data + neg_data
labels = np.concatenate((pos['rating'].values,neg['rating'].values))
#print(labels)
[Data_train,Data_test,Train_labels,Test_labels] = train_test_split(data,labels , test_size=0.25, random_state=20160121,stratify=labels)
t = []
for line in data:
    l = nltk.word_tokenize(line)
    for w in l:
            t.append(w)
    #print(t)
word_features = nltk.FreqDist(t)
print(len(word_features))
topwords = [fpair[0] for fpair in list(word_features.most_common(5000))]
print(word_features.most_common(25))
word_his = pd.DataFrame(word_features.most_common(200), columns = ['words','count'])
    #print(word_his)
vec = CountVectorizer()
c_fit = vec.fit_transform([' '.join(topwords)])
print(c_fit)
tf_vec = TfidfTransformer()
tf_fit = tf_vec.fit_transform(c_fit)
ctr_features = vec.transform(data)
tr_features = tf_vec.transform(ctr_features)
tr_features.shape
#cte_features = vec.transform(Data_test)
#te_features = tf_vec.transform(cte_features)
clf = DecisionTreeClassifier()
clf = clf.fit(tr_features, labels)
tfPredication = clf.predict(tr_features)
tfAccuracy = metrics.accuracy_score(tfPredication,labels)
print(tfAccuracy)
print(metrics.classification_report(labels, tfPredication))
reviews = pd.read_csv('C:\Users\Ajinkya\Desktop/amazon_baby_test.csv')
reviews.shape
reviews = reviews.dropna()
reviews.shape
#print(reviews.head(25))
scores = reviews['rating']
reviews['rating'] = reviews['rating'].apply(lambda x: 'pos' if x > 3 else 'neg')
#print(reviews.head(25))
scores.mean()
reviews.groupby('rating')['review'].count()
reviews.groupby('rating')['review'].count().plot(kind='bar', color= ['r','g'],title='Label Distribution',  figsize = (10,6))
[pos,neg] = splitPosNeg(reviews)
pos_data = []
neg_data = []
for p in pos['review']:
   pos_data.append(preprocessing(p))
for n in neg['review']:
    neg_data.append(preprocessing(n))
data = pos_data + neg_data
labels = np.concatenate((pos['rating'].values,neg['rating'].values))
    #print(labels)
t = []
for line in data:
        l = nltk.word_tokenize(line)
        for w in l:
            t.append(w)
    #print(t)
word_features = nltk.FreqDist(t)
print(len(word_features))
topwords = [fpair[0] for fpair in list(word_features.most_common(5002))]
print(word_features.most_common(25))
word_his = pd.DataFrame(word_features.most_common(200), columns = ['words','count'])
    #print(word_his)
len(topwords)
vec = CountVectorizer()
c_fit = vec.fit_transform([' '.join(topwords)])
tf_vec = TfidfTransformer()
tf_fit = tf_vec.fit_transform(c_fit)
cte_features = vec.transform(data)
te_features = tf_vec.transform(cte_features)
te_features.shape
tePredication = clf.predict(te_features)
teAccuracy = metrics.accuracy_score(tePredication,labels)
print(teAccuracy)
print(metrics.classification_report(labels, tePredication))